{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-playlist",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "# RDD\n",
    "\n",
    "## TP2_Exercice1_RDD\n",
    "\n",
    "- Introduction à pyspark\n",
    "- Introduction du RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adjusted-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction sur les RDD\n",
    "# import des librairies bumpy et py spark\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "# add est la fonction addition\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "outdoor-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des élements du RDD :  [4, 4, 4, 1, 2, 9, 7, 3, 4, 9, 7, 5, 8, 9, 6, 0, 1, 5, 5, 2]\n",
      "Nbre d'éléments de RDD_A :  20\n",
      "Type de RDD :  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "Liste des élements du RDD par partitions :  [[4, 4, 4, 1, 2], [9, 7, 3, 4, 9], [7, 5, 8, 9, 6], [0, 1, 5, 5, 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialisation de Spark Context 4 partitions\n",
    "# Dans ce notebook, on utilisera SparkContext\n",
    "# On verra que ce n'est pas la seule façon de faire.\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "# Initialisation d'un vecteur de 20 nbre aléatoires (entiers compris entre 0 et 9 inclus)\n",
    "# utilisation d'une fonction numpy\n",
    "A = np.random.randint(0,10,20)\n",
    "\n",
    "# Creation d'un RDD parallelisé à partir du vecteur précédent  \n",
    "# Le RDD est distribué sur les 4 partitions\n",
    "RDD_A = sc.parallelize(A)\n",
    "\n",
    "# Affichage fonction .collect()\n",
    "# .collect renvoie une liste de tous les éléments du RDD (qui peut être afficher)\n",
    "print(\"Liste des élements du RDD : \",RDD_A.collect())\n",
    "\n",
    "# Comptons le nombre d'éléments\n",
    "print(\"Nbre d'éléments de RDD_A : \",RDD_A.count())\n",
    "# On retouve 20\n",
    "\n",
    "# print(RDD_A) va juste indiquer le type du RDD\n",
    "print(\"Type de RDD : \",RDD_A)\n",
    "\n",
    "# Affichage des partitions (grâce à la fonction glom())\n",
    "# .glom() va fusionner en considérant les élément de chaque partition\n",
    "# .collect crée une liste \n",
    "# On remarque les 4 partitions\n",
    "print(\"Liste des élements du RDD par partitions : \", RDD_A.glom().collect())\n",
    "\n",
    "# on n'oublie d'arrêter le spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-wrist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des élements du RDD :  [7, 2, 4, 3, 1, 9, 9, 8, 9, 6, 9, 9, 6, 4, 4, 5, 0, 3, 5, 8]\n",
      "Type de RDD :  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "Liste des élements du RDD par partitions :  [[7, 2, 4, 3, 1, 9, 9, 8, 9, 6], [9, 9, 6, 4, 4, 5, 0, 3, 5, 8]]\n"
     ]
    }
   ],
   "source": [
    "# On recommence la même chose avec 2 partitions\n",
    "# On remarque qu'il n'y pplus \n",
    "# Initialisation de Spark Context 2 partitions\n",
    "sc = SparkContext(master=\"local[2]\")\n",
    "\n",
    "# Initialisation d'un vecteur de 20 nbre aléatoires (entiers)\n",
    "A = np.random.randint(0,10,20)\n",
    "\n",
    "# Creation d'un RDD parallelise a partir du vecteur précédent  \n",
    "RDD_A = sc.parallelize(A)\n",
    "\n",
    "# Affichage\n",
    "print(\"Liste des élements du RDD : \",RDD_A.collect())\n",
    "\n",
    "# print(RDD_A) va juste indiquer le type du RDD\n",
    "print(\"Type de RDD : \",RDD_A)\n",
    "\n",
    "# Affichage des partitions\n",
    "# Ici, on remarque les deux partitions\n",
    "print(\"Liste des élements du RDD par partitions : \",RDD_A.glom().collect())\n",
    "\n",
    "# on n'oublie d'arrêter le spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-bearing",
   "metadata": {},
   "source": [
    "L'intégralité des fonctions applicables aux RDD se retrouve à la page :\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "    \n",
    "On peut y rescencer une centaine de fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vertical-particle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des élements du RDD :  [4, 7, 3, 8, 9, 0, 0, 5, 1, 4, 0, 4, 4, 7, 7, 6, 2, 2, 0, 0]\n",
      "\n",
      "Liste des élemnts (unique) de RDD_A :  [4, 8, 0, 9, 5, 1, 6, 2, 7, 3]\n",
      "\n",
      "Somme des élements de RDD_A\n",
      "Utilisation de sum() :  73\n",
      "Utilisation de reduce(lambda x,y:x+y) :  73\n",
      "Utilisation de fold(0,lambda x,y:x+y) :  73\n",
      "Utilisation de fold(0,add) :  73\n",
      "\n",
      "Affichege des nbre pairs :  [4, 8, 0, 0, 4, 0, 4, 4, 6, 2, 2, 0, 0]\n",
      "\n",
      " min :  0  ; max :  9  ; stdev : 2.903015673398957\n",
      " utilisation de stats() :  (count: 20, mean: 3.65, stdev: 2.903015673398957, max: 9.0, min: 0.0)\n",
      "\n",
      "Mise au carré du RDD \n",
      "Utilisation de map(lambda x:x*x ) :  [16, 49, 9, 64, 81, 0, 0, 25, 1, 16, 0, 16, 16, 49, 49, 36, 4, 4, 0, 0]\n",
      "définition de square_x Utilisation de map(square_x ) :  [16, 49, 9, 64, 81, 0, 0, 25, 1, 16, 0, 16, 16, 49, 49, 36, 4, 4, 0, 0]\n",
      "\n",
      "Ici, on crée (x,1) :  [(4, 1), (7, 1), (3, 1), (8, 1), (9, 1), (0, 1), (0, 1), (5, 1), (1, 1), (4, 1), (0, 1), (4, 1), (4, 1), (7, 1), (7, 1), (6, 1), (2, 1), (2, 1), (0, 1), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de Spark Context 4 partitions\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "# Initialisation d'un vecteur de 20 nbre aléatoires (entierer)\n",
    "A = np.random.randint(0,10,20)\n",
    "\n",
    "# Creation d'un RDD parallelise a partir du vecteur précédent  \n",
    "RDD_A = sc.parallelize(A)\n",
    "\n",
    "# Affichage\n",
    "print(\"Liste des élements du RDD : \",RDD_A.collect())\n",
    "\n",
    "# Création d'un vecteur avec des valeurs uniques\n",
    "RDD_A_Distinct = RDD_A.distinct()\n",
    "\n",
    "print(\"\")\n",
    "# Affichage\n",
    "# Les éléments de RDD_A_Distinct ne sont pas triés\n",
    "print(\"Liste des élemnts (unique) de RDD_A : \",RDD_A_Distinct.collect())\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "# Somme des valeur de RDD_A\n",
    "# Nous allons présenter 4 méthodes \n",
    "print(\"Somme des élements de RDD_A\")\n",
    "# Utilisation de la fonction sum()\n",
    "print(\"Utilisation de sum() : \",RDD_A.sum())\n",
    "\n",
    "# utilisation de la fonction reduce avec appliquant x + y\n",
    "print(\"Utilisation de reduce(lambda x,y:x+y) : \",RDD_A.reduce(lambda x,y:x+y))\n",
    "\n",
    "# utilisation de fold\n",
    "# fold s'applique d'abord sur chaque partition puis effectue une réduction de l'ensemble des partitions réduites\n",
    "# 0 est un valeur neutre\n",
    "print(\"Utilisation de fold(0,lambda x,y:x+y) : \",RDD_A.fold(0,lambda x,y:x+y))\n",
    "\n",
    "# utilisation de fold et de l'opérateur add\n",
    "print(\"Utilisation de fold(0,add) : \",RDD_A.fold(0,add))\n",
    "\n",
    "# On remarque que les résultats sont identiques\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# filtrer en ne retenant que les nbres pairs\n",
    "# Filter renvoie un nouveau RDD contenant uniquement les éléments qui satisfont à un prédicat.\n",
    "# ici le prédicat est x%2==0 \n",
    "print(\"Affichage des nbres pairs : \", RDD_A.filter(lambda x:x%2==0 ).collect())\n",
    "\n",
    "print(\"\")\n",
    "# Affichge de quelques fonctions statistiques\n",
    "# affichage du min, max, écart type, des stats\n",
    "print(\" min : \",RDD_A.min(),\" ; max : \", RDD_A.max(),\" ; stdev :\", RDD_A.stdev())\n",
    "print(\" utilisation de stats() : \",RDD_A.stats())\n",
    "\n",
    "print(\"\")\n",
    "# faire un map en mettant au carre\n",
    "# a chaque élement du rdd, j'applique la fonction définie\n",
    "print(\"Mise au carré du RDD \")\n",
    "RDD_B = RDD_A.map(lambda x:x*x )\n",
    "\n",
    "# afficher\n",
    "print(\"Utilisation de map(lambda x:x*x ) : \",RDD_B.collect())\n",
    "\n",
    "# définir une fonction\n",
    "def square_x(x):\n",
    "   return x*x\n",
    "\n",
    "# faire un map en appliquant une fonction\n",
    "RDD_B2 = RDD_A.map(square_x)\n",
    "\n",
    "#Affichage\n",
    "print(\"définition de square_x Utilisation de map(square_x ) : \", RDD_B2.collect())\n",
    "\n",
    "print(\"\")\n",
    "# La fonction map es ttrès très souvent utilisée\n",
    "# faire un map en mettant au carre\n",
    "# a chaque élement du rdd, j'applique la fonction définie\n",
    "RDD_BB = RDD_A.map(lambda x:(x,1) )\n",
    "\n",
    "#Affichage\n",
    "print(\"Ici, on crée (x,1) : \", RDD_BB.collect())\n",
    "\n",
    "\n",
    "# on n'oublie d'arrêter le spark context\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "plain-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD_C :  [3, 5]\n",
      "RDD_D :  [5, 5, 1]\n",
      "Fusion à l'aide de + :  [3, 5, 5, 5, 1]\n",
      "Création de tous les couples possibles (c_i, d_j) :  [(3, 5), (3, 5), (3, 1), (5, 5), (5, 5), (5, 1)]\n",
      "Création de tous les couples possibles (c_i, c_j) :  [(3, 3), (3, 5), (5, 3), (5, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de Spark Context 4 partitions\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "\n",
    "# On va maintenant combiner 2 RDD\n",
    "\n",
    "# créer 2 vecteurs aléatoires puis les RDD associés\n",
    "C = np.random.randint(0,10,2)\n",
    "RDD_C = sc.parallelize(C)\n",
    "D = A = np.random.randint(0,10,3)\n",
    "RDD_D = sc.parallelize(D)\n",
    "print(\"RDD_C : \",RDD_C.collect())\n",
    "print(\"RDD_D : \",RDD_D.collect())\n",
    "\n",
    "# Fusionner les 2 RDD et afficher\n",
    "print(\"Fusion à l'aide de + : \",(RDD_C+RDD_D).collect())\n",
    "\n",
    "# Former tous les couples possible (c_i, d_j)\n",
    "print(\"Création de tous les couples possibles (c_i, d_j) : \", RDD_C.cartesian(RDD_D).collect())\n",
    "# Former tous les couples possible (c_i, c_j)\n",
    "print(\"Création de tous les couples possibles (c_i, c_j) : \", RDD_C.cartesian(RDD_C).collect())\n",
    "# on n'oublie d'arrêter le spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "exact-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En cas d'erreur de codage, il faut arrêter sc avant de relancer\n",
    "# Dans ce cas la, excécuter cette ligne, puis revenez à votre cellule\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-strategy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
